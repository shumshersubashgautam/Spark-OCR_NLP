{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of running OCR in streaming mode for process PDF's\n",
    "## Install spark-ocr python packge\n",
    "Need specify path to `spark-ocr-assembly-[version].jar` or `secret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = \"\"\n",
    "license = \"\"\n",
    "version = secret.split(\"-\")[0]\n",
    "spark_ocr_jar_path = \"../../target/scala-2.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if python -c 'import google.colab' &> /dev/null; then\n",
    "    echo \"Run on Google Colab!\"\n",
    "    echo \"Install Open JDK\"\n",
    "    apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "    java -version\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install from PYPI using secret\n",
    "#%pip install spark-ocr==$version+spark30 --extra-index-url=https://pypi.johnsnowlabs.com/$secret --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or install from local path\n",
    "# %pip install ../../python/dist/spark-ocr-1.9.0.spark24.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.1\n",
      "Spark NLP version: 4.0.0\n",
      "Spark NLP for Healthcare version: 4.0.0\n",
      "Spark OCR version: 4.0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark OCR</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4f6f7f41d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sparkocr import start\n",
    "\n",
    "if license:\n",
    "    os.environ['JSL_OCR_LICENSE'] = license\n",
    "\n",
    "spark = start(secret=secret, jar_path=spark_ocr_jar_path)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from sparkocr.transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill path to folder with PDF's here\n",
    "dataset_path = \"data/pdfs/*.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one file for infer schema\n",
    "pdfs_df = spark.read.format(\"binaryFile\").load(dataset_path).limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define OCR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform binary to image\n",
    "pdf_to_image = PdfToImage()\n",
    "pdf_to_image.setOutputCol(\"image\")\n",
    "\n",
    "# Run OCR for each region\n",
    "ocr = ImageToText()\n",
    "ocr.setInputCol(\"image\")\n",
    "ocr.setOutputCol(\"text\")\n",
    "ocr.setConfidenceThreshold(60)\n",
    "\n",
    "# OCR pipeline\n",
    "pipeline = PipelineModel(stages=[\n",
    "    pdf_to_image,\n",
    "    ocr\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define streaming pipeline and start it\n",
    "Note: each start erase previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of files in one microbatch\n",
    "maxFilesPerTrigger = 4 \n",
    "\n",
    "# read files as stream\n",
    "pdf_stream_df = spark.readStream \\\n",
    ".format(\"binaryFile\") \\\n",
    ".schema(pdfs_df.schema) \\\n",
    ".option(\"maxFilesPerTrigger\", maxFilesPerTrigger) \\\n",
    ".load(dataset_path)\n",
    "\n",
    "# process files using OCR pipoeline\n",
    "result = pipeline.transform(pdf_stream_df).withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# store results to memory table\n",
    "query = result.writeStream \\\n",
    " .format('memory') \\\n",
    " .queryName('result') \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get progress of streamig job\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run for stop steraming job\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results from 'result' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of processed records (number of processed pages in results)\n",
    "spark.table(\"result\").count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+\n",
      "|           timestamp|pagenum|                path|                text|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "|2022-07-14 17:41:...|      0|file:/home/jose/s...| \\n\\n \\n\\n \\n\\nne...|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "spark.table(\"result\").select(\"timestamp\",\"pagenum\", \"path\", \"text\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run streaming job for storing results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = result.select(\"text\").writeStream \\\n",
    " .format('text') \\\n",
    " .option(\"path\", \"results/\") \\\n",
    " .option(\"checkpointLocation\", \"checkpointDir\") \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '843edde6-9e8e-4fd3-ba8a-5f7cc3ef5de0',\n",
       " 'runId': '2d9fab9d-1aa7-4d6f-bef8-1f82840ef0b3',\n",
       " 'name': None,\n",
       " 'timestamp': '2022-07-14T20:43:36.505Z',\n",
       " 'batchId': 1,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/home/jose/spark-ocr/workshop/jupyter/data/pdfs/*.pdf]',\n",
       "   'startOffset': {'logOffset': 0},\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'latestOffset': None,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'FileSink[results/]', 'numOutputRows': -1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get progress of streamig job\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run for stop steraming job\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read results from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|ne Pa a Date: 7/1...|\n",
      "|er â€˜Sample No. _ ...|\n",
      "|â€œ Original reques...|\n",
      "|                    |\n",
      "|Sample specificat...|\n",
      "| , BLEND CASING R...|\n",
      "|                    |\n",
      "|- OLD GOLD STRAIG...|\n",
      "|                    |\n",
      "|Control for Sampl...|\n",
      "|                    |\n",
      "|         Cigarettes:|\n",
      "|                    |\n",
      "|   OLD GOLD STRAIGHT|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_SKIP\n",
    "results = spark.read.format(\"text\").load(\"results/*.txt\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean results and checkpoint folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r -f results\n",
    "rm -r -f checkpointDir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
